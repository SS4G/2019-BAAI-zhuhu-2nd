{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['member_info_0926.txt',\n",
       " 'question_info_0926.txt',\n",
       " 'word_vectors_64d.txt',\n",
       " 'invite_info_0926.txt',\n",
       " 'answer_info_0926.txt',\n",
       " 'topic_vectors_64d.txt',\n",
       " 'invite_info_evaluate_2_0926.txt',\n",
       " 'single_word_vectors_64d.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'invite_info_evaluate_1_0926.txt',\n",
       " 'sample_sub_1.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold,GroupKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.set_random_seed(1024)\n",
    "path=\"../data/\"\n",
    "os.listdir(\"../data/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"out\"):\n",
    "    os.mkdir(\"out\")\n",
    "x1_te=np.load(\"./out/x1_te.npy\")\n",
    "x2_te=np.load(\"./out/x2_te.npy\")\n",
    "x3_te=np.load(\"./out/x3_te.npy\")\n",
    "x4_te=np.load(\"./out/x4_te.npy\")\n",
    "x5_te=np.load(\"./out/x5_te.npy\")\n",
    "x6_te=np.load(\"./out/x6_te.npy\")\n",
    "test_fe=np.load(\"./out/test_fe.npy\")\n",
    "\n",
    "emb1 = np.load(\"./out/emb1.npy\")\n",
    "emb2 = np.load(\"./out/emb2.npy\")\n",
    "emb3 = np.load(\"./out/emb3.npy\")\n",
    "emb4 = np.load(\"./out/emb4.npy\")\n",
    "emb5 = np.load(\"./out/emb5.npy\")\n",
    "emb6 = np.load(\"./out/emb6.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要用到的函数\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            # decoupled weight decay (2/4)\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay')\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd  # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            # decoupled weight decay (4/4)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1,emb2, emb3,emb4,emb5,emb6):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=88,\n",
    "        trainable=False\n",
    "    )\n",
    "    emb_layer_2 = Embedding(\n",
    "        input_dim=emb2.shape[0],\n",
    "        output_dim=emb2.shape[1],\n",
    "        weights=[emb2],\n",
    "        input_length=6,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=57,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_4 = Embedding(\n",
    "        input_dim=emb4.shape[0],\n",
    "        output_dim=emb4.shape[1],\n",
    "        weights=[emb4],\n",
    "        input_length=17,\n",
    "        trainable=False\n",
    "    )\n",
    "    emb_layer_5 = Embedding(\n",
    "        input_dim=emb5.shape[0],\n",
    "        output_dim=emb5.shape[1],\n",
    "        weights=[emb5],\n",
    "        input_length=5,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_6 = Embedding(\n",
    "        input_dim=emb6.shape[0],\n",
    "        output_dim=emb6.shape[1],\n",
    "        weights=[emb6],\n",
    "        input_length=17,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(88,))\n",
    "    seq2 = Input(shape=(6,))\n",
    "    seq3 = Input(shape=(57,))  \n",
    "    seq4 = Input(shape=(17,)) \n",
    "    seq5 = Input(shape=(5,)) \n",
    "    seq6 = Input(shape=(17,)) \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x2 = emb_layer_2(seq2)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    x4 = emb_layer_4(seq4)\n",
    "    x5 = emb_layer_5(seq5)\n",
    "    x6 = emb_layer_6(seq6)\n",
    "    \n",
    "    sdrop= SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x2 = sdrop(x2)\n",
    "    x3 = sdrop(x3)\n",
    "    x4 = sdrop(x4)\n",
    "    x5 = sdrop(x5)\n",
    "    x6 = sdrop(x6)\n",
    "    \n",
    "    \n",
    "    lstm_layer1 = Bidirectional(CuDNNLSTM(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    gru_layer1 = Bidirectional(CuDNNGRU(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    \n",
    "    lstm_layer2 = Bidirectional(CuDNNLSTM(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    gru_layer2 = Bidirectional(CuDNNGRU(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    \n",
    "    lstm_layer3 = Bidirectional(CuDNNLSTM(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    gru_layer3 = Bidirectional(CuDNNGRU(64, return_sequences=True, \n",
    "kernel_initializer=glorot_uniform(seed = 123)))\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()\n",
    "    max_pool = GlobalMaxPooling1D()\n",
    "    \n",
    "    x1_lstm = lstm_layer1(x1)\n",
    "    x1_gru = gru_layer1(x1_lstm)\n",
    "    x1_att = Attention(88)(x1_gru)\n",
    "    \n",
    "    x2_lstm = lstm_layer2(x2)\n",
    "    x2_gru = gru_layer2(x2_lstm)\n",
    "    x2_att = Attention(6)(x2_gru)\n",
    "    \n",
    "    x3_lstm = lstm_layer2(x3)\n",
    "    x3_gru = gru_layer2(x3_lstm)\n",
    "    x3_att = Attention(57)(x3_gru)\n",
    "    \n",
    "    x4_lstm = lstm_layer2(x4)\n",
    "    x4_gru = gru_layer2(x4_lstm)\n",
    "    x4_att = Attention(17)(x4_gru)\n",
    "    \n",
    "    x5_lstm = lstm_layer2(x5)\n",
    "    x5_gru = gru_layer2(x5_lstm)\n",
    "    x5_att = Attention(5)(x5_gru)\n",
    "    \n",
    "    x6_lstm = lstm_layer3(x6)\n",
    "    x6_gru = gru_layer3(x6_lstm)\n",
    "    x6_att = Attention(17)(x6_gru)\n",
    "    \n",
    "    x1_feature = concatenate([x1_att,avg_pool(x1_gru),max_pool(x1_gru)])\n",
    "    x2_feature = concatenate([x2_att,avg_pool(x2_gru),max_pool(x2_gru)]) \n",
    "    x3_feature = concatenate([x3_att,avg_pool(x3_gru),max_pool(x3_gru)]) \n",
    "    x4_feature = concatenate([x4_att,avg_pool(x4_gru),max_pool(x4_gru)]) \n",
    "    x5_feature = concatenate([x5_att,avg_pool(x5_gru),max_pool(x5_gru)]) \n",
    "    x6_feature = concatenate([x6_att,avg_pool(x6_gru),max_pool(x6_gru)]) \n",
    "    \n",
    "    merge1 = Multiply()([x2_feature, x3_feature])\n",
    "    merge2 = Multiply()([x2_feature, x5_feature])\n",
    "    merge3 = Multiply()([x4_feature, x6_feature])\n",
    "    \n",
    "#     merge1 = esim(x2,x3)\n",
    "#     merge2 = esim(x2,x4)\n",
    "#     merge3 = esim(x2,x5)\n",
    "    \n",
    "    hin = Input(shape=(101, ))\n",
    "    htime = Dense(101, activation='relu')(hin)\n",
    "    \n",
    "    x = concatenate([merge1,merge2,merge3,x1_feature,x2_feature,x3_feature,x4_feature,x5_feature,x6_feature, htime])\n",
    "    \n",
    "    x = Dense(128,kernel_initializer=he_uniform(seed=123), activation='relu',)(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[seq1,seq2,seq3,seq4,seq5,seq6, hin], outputs=pred)\n",
    "    from keras.utils import multi_gpu_model\n",
    "#     model = multi_gpu_model(model, 4)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=AdamW(lr=0.001,weight_decay=0.02,),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 57)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 6, 300)       16377600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 57, 300)      18529500    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 5, 300)       12228300    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 17, 300)      23483100    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 17, 300)      38446200    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 88)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro multiple             0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 88, 300)      23718000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) multiple             187392      spatial_dropout1d_1[1][0]        \n",
      "                                                                 spatial_dropout1d_1[2][0]        \n",
      "                                                                 spatial_dropout1d_1[3][0]        \n",
      "                                                                 spatial_dropout1d_1[4][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 17, 128)      187392      spatial_dropout1d_1[5][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) multiple             74496       bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_3[1][0]            \n",
      "                                                                 bidirectional_3[2][0]            \n",
      "                                                                 bidirectional_3[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 17, 128)      74496       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 88, 128)      187392      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          134         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "                                                                 bidirectional_4[2][0]            \n",
      "                                                                 bidirectional_4[3][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "                                                                 bidirectional_4[2][0]            \n",
      "                                                                 bidirectional_4[3][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          185         bidirectional_4[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 128)          133         bidirectional_4[3][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          145         bidirectional_4[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          145         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 88, 128)      74496       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_1[1][0] \n",
      "                                                                 global_max_pooling1d_1[1][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 384)          0           attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_1[2][0] \n",
      "                                                                 global_max_pooling1d_1[2][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_1[4][0] \n",
      "                                                                 global_max_pooling1d_1[4][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           attention_4[0][0]                \n",
      "                                                                 global_average_pooling1d_1[3][0] \n",
      "                                                                 global_max_pooling1d_1[3][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 384)          0           attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_1[5][0] \n",
      "                                                                 global_max_pooling1d_1[5][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          216         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 384)          0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 384)          0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 384)          0           concatenate_4[0][0]              \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 101)          10302       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 3557)         0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "                                                                 concatenate_6[0][0]              \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          455424      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 134,035,177\n",
      "Trainable params: 1,252,477\n",
      "Non-trainable params: 132,782,700\n",
      "__________________________________________________________________________________________________\n",
      "2283401/2283401 [==============================] - 155s 68us/step\n",
      "2283401/2283401 [==============================] - 155s 68us/step\n",
      "2283401/2283401 [==============================] - 158s 69us/step\n",
      "2283401/2283401 [==============================] - 154s 68us/step\n",
      "2283401/2283401 [==============================] - 156s 68us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "test_pred=0\n",
    "for i in range(5):\n",
    "    model_path=\"out/new_final_all_data_self_w2v_nn_chizhu_{}.h5\".format(i)\n",
    "    model = model_conv(emb1, emb2,emb3,emb4,emb5,emb6)\n",
    "    model.load_weights(model_path)\n",
    "    if i==0:model.summary()\n",
    "    test_pred += np.squeeze(model.predict([x1_te,x2_te, x3_te,x4_te, x5_te,x6_te,test_fe],batch_size=2048,verbose=1)/5)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv(\"./out/invite_info_evaluate_2_0926.txt\",sep=\"\\t\",names=[\"qid\",\"mid\",\"time\"])\n",
    "sub['label']=test_pred[1141683:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"submit\"):\n",
    "    os.mkdir(\"submit\")\n",
    "sub.to_csv(\"nn_testB.txt\",sep=\"\\t\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13132545351982117"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['label'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
