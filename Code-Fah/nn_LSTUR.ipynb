{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embedding= np.load( \"esim_topic_embedding.npy\" )\n",
    "word_embedding= np.load( \"esim_word_embedding.npy\" )\n",
    "\n",
    "qid_title=np.load(\"qtitlec.npy\")\n",
    "qid_1_title=np.load(\"qtitlec_1.npy\")\n",
    "qid_2_title=np.load(\"qtitlec_2.npy\")\n",
    "qid_3_title=np.load(\"qtitlec_3.npy\")\n",
    "qid_4_title=np.load(\"qtitlec_4.npy\")\n",
    "qid_5_title=np.load(\"qtitlec_5.npy\")\n",
    "qid_6_title=np.load(\"qtitlec_6.npy\")\n",
    "\n",
    "qid_topic=np.load(\"qtopic.npy\")\n",
    "qid_1_topic=np.load(\"qtopic_1.npy\")\n",
    "qid_2_topic=np.load(\"qtopic_2.npy\")\n",
    "qid_3_topic=np.load(\"qtopic_3.npy\")\n",
    "qid_4_topic=np.load(\"qtopic_4.npy\")\n",
    "qid_5_topic=np.load(\"qtopic_5.npy\")\n",
    "qid_6_topic=np.load(\"qtopic_6.npy\")\n",
    "l_topic=np.load(\"l_topic.npy\")\n",
    "u_topic=np.load(\"u_topic.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.load(\"all_data_final.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unchanged_shape(input_shape):\n",
    "    \"Function for Lambda layer\"\n",
    "    return input_shape\n",
    "def compare(input_1, input_2):\n",
    "    difft = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=unchanged_shape)([input_1, input_2])\n",
    "    mult = Lambda(lambda x: x[0] * x[1], output_shape=unchanged_shape)([input_1, input_2])\n",
    "    output=concatenate([difft, mult])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shape=feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LSTUR(titlec_len=10,topic_len=10,word_embedding=word_embedding,topic_embedding_=topic_embedding):\n",
    "    \n",
    "    word_embedding = Embedding(\n",
    "        input_dim=word_embedding.shape[0],\n",
    "        output_dim=word_embedding.shape[1],\n",
    "        weights=[word_embedding],\n",
    "        input_length=titlec_len,\n",
    "        trainable=False\n",
    "    ) \n",
    "    \n",
    "    topic_embedding = Embedding(\n",
    "        input_dim=topic_embedding_.shape[0],\n",
    "        output_dim=topic_embedding_.shape[1],\n",
    "        weights=[topic_embedding_],\n",
    "        input_length=topic_len,\n",
    "        trainable=False\n",
    "    ) \n",
    "    \n",
    "    qid_title   = Input(shape=(titlec_len,), name=\"qid_title\")\n",
    "    qid_1_title = Input(shape=(titlec_len,), name=\"qid_1_title\")\n",
    "    qid_2_title = Input(shape=(titlec_len,), name=\"qid_2_title\")\n",
    "    qid_3_title = Input(shape=(titlec_len,), name=\"qid_3_title\")\n",
    "    qid_4_title = Input(shape=(titlec_len,), name=\"qid_4_title\")\n",
    "    qid_5_title = Input(shape=(titlec_len,), name=\"qid_5_title\")\n",
    "    qid_6_title = Input(shape=(titlec_len,), name=\"qid_6_title\")\n",
    "    \n",
    "    qid_topic   = Input(shape=(topic_len,), name=\"qid_topic\")\n",
    "    qid_1_topic = Input(shape=(topic_len,), name=\"qid_1_topic\")\n",
    "    qid_2_topic = Input(shape=(topic_len,), name=\"qid_2_topic\")\n",
    "    qid_3_topic = Input(shape=(topic_len,), name=\"qid_3_topic\")\n",
    "    qid_4_topic = Input(shape=(topic_len,), name=\"qid_4_topic\")\n",
    "    qid_5_topic = Input(shape=(topic_len,), name=\"qid_5_topic\")\n",
    "    qid_6_topic = Input(shape=(topic_len,), name=\"qid_6_topic\")\n",
    "    \n",
    "    l_topic = Input(shape=(50,), name=\"l_topic\")\n",
    "    \n",
    "    ltopic_embedding = Embedding(\n",
    "        input_dim=topic_embedding_.shape[0],\n",
    "        output_dim=topic_embedding_.shape[1],\n",
    "        weights=[topic_embedding_],\n",
    "        input_length=50,\n",
    "        trainable=False\n",
    "    ) \n",
    "    l_topic_emb=ltopic_embedding(l_topic)\n",
    "    \n",
    "    conv   = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_1 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_2 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_3 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_4 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_5 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    conv_6 = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')\n",
    "    \n",
    "    qid_title_emb=word_embedding(qid_title)\n",
    "    qid_1_title_emb=word_embedding(qid_1_title)\n",
    "    qid_2_title_emb=word_embedding(qid_2_title)\n",
    "    qid_3_title_emb=word_embedding(qid_3_title)\n",
    "    qid_4_title_emb=word_embedding(qid_4_title)\n",
    "    qid_5_title_emb=word_embedding(qid_5_title)\n",
    "    qid_6_title_emb=word_embedding(qid_6_title)\n",
    "    \n",
    "    \n",
    "    qid_topic_emb=topic_embedding(qid_topic)\n",
    "    qid_1_topic_emb=topic_embedding(qid_1_topic)\n",
    "    qid_2_topic_emb=topic_embedding(qid_2_topic)\n",
    "    qid_3_topic_emb=topic_embedding(qid_3_topic)\n",
    "    qid_4_topic_emb=topic_embedding(qid_4_topic)\n",
    "    qid_5_topic_emb=topic_embedding(qid_5_topic)\n",
    "    qid_6_topic_emb=topic_embedding(qid_6_topic)\n",
    "    \n",
    "    qid_title_rep=conv(qid_title_emb)\n",
    "    qid_1_title_rep=conv_1(qid_1_title_emb)\n",
    "    qid_2_title_rep=conv_2(qid_2_title_emb)\n",
    "    qid_3_title_rep=conv_3(qid_3_title_emb)\n",
    "    qid_4_title_rep=conv_4(qid_4_title_emb)\n",
    "    qid_5_title_rep=conv_5(qid_5_title_emb)\n",
    "    qid_6_title_rep=conv_6(qid_6_title_emb)\n",
    "    \n",
    "    qid_title_rep   = Attention(titlec_len)(qid_title_rep)\n",
    "    qid_1_title_rep = Attention(titlec_len)(qid_1_title_rep)\n",
    "    qid_2_title_rep = Attention(titlec_len)(qid_2_title_rep)\n",
    "    qid_3_title_rep = Attention(titlec_len)(qid_3_title_rep)\n",
    "    qid_4_title_rep = Attention(titlec_len)(qid_4_title_rep)\n",
    "    qid_5_title_rep = Attention(titlec_len)(qid_5_title_rep)\n",
    "    qid_6_title_rep = Attention(titlec_len)(qid_6_title_rep)\n",
    " #   return 0\n",
    "    qtitle_history =concatenate([qid_1_title_rep,qid_2_title_rep,qid_3_title_rep,qid_4_title_rep,qid_5_title_rep,qid_6_title_rep])\n",
    "    qtitle_history=Reshape((6,64))(qtitle_history)\n",
    "    GRU_Encode1=Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "    GRU_Encode2=Bidirectional(CuDNNGRU(32))\n",
    "    qtitle_history=GRU_Encode1(qtitle_history)\n",
    "    qtitle_history=GRU_Encode2(qtitle_history)\n",
    "    qtopic_history=concatenate([qid_1_topic_emb,qid_2_topic_emb,qid_3_topic_emb,qid_4_topic_emb,qid_5_topic_emb,qid_6_topic_emb],axis=1)\n",
    "    \n",
    "    u_rep=concatenate([qtitle_history,GlobalAveragePooling1D()(qtopic_history),GlobalAveragePooling1D()(l_topic_emb)])\n",
    "    \n",
    "    q_rep=concatenate([qid_title_rep,GlobalAveragePooling1D()(qid_topic_emb),GlobalAveragePooling1D()(qid_topic_emb)])\n",
    "    feature = Input(shape=(feature_shape,), name=\"feature\")\n",
    "    feature_fc= Dense(1024, activation='relu', )(feature)\n",
    "    feature_fc = Dropout(0.3)(feature_fc)\n",
    "    feature_fc = Dense(512, activation='relu', )(feature_fc)\n",
    "    feature_fc = Dropout(0.3)(feature_fc)\n",
    "    out2=compare(u_rep, q_rep)\n",
    "    out=concatenate([out2,feature_fc])\n",
    "    out = Dense(1024,activation='relu')(out)\n",
    "    feature_fc = Dropout(0.3)(feature_fc)\n",
    "    out = Dense(512,activation='relu')(out)\n",
    "    output = Dense(1,activation='sigmoid')(out)\n",
    "    \n",
    "    model = Model(inputs=[qid_title,qid_1_title,qid_2_title,qid_3_title,qid_4_title,qid_5_title,qid_6_title,\n",
    "                          qid_topic,qid_1_topic,qid_2_topic,qid_3_topic,qid_4_topic,qid_5_topic,qid_6_topic,\n",
    "                          l_topic,feature], outputs=output)\n",
    "\n",
    "    model.compile(optimizer ='adam',loss= 'binary_crossentropy',metrics=['acc'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "#LSTUR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_1.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "target=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = np.zeros((test.shape[0],1 ))\n",
    "oof_pref1 = np.zeros((train.shape[0], 1))\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(index_):\n",
    "    data=dict()\n",
    "    data['qid_title']=qid_title[index_]\n",
    "    data['qid_1_title']=qid_1_title[index_]\n",
    "    data['qid_2_title']=qid_2_title[index_]\n",
    "    data['qid_3_title']=qid_3_title[index_]\n",
    "    data['qid_4_title']=qid_4_title[index_]\n",
    "    data['qid_5_title']=qid_5_title[index_]\n",
    "    data['qid_6_title']=qid_6_title[index_]\n",
    "    data['qid_topic']=qid_topic[index_]\n",
    "    data['qid_1_topic']=qid_1_topic[index_]\n",
    "    data['qid_2_topic']=qid_2_topic[index_]\n",
    "    data['qid_3_topic']=qid_3_topic[index_]\n",
    "    data['qid_4_topic']=qid_4_topic[index_]\n",
    "    data['qid_5_topic']=qid_5_topic[index_]\n",
    "    data['qid_6_topic']=qid_6_topic[index_]\n",
    "    data['l_topic']=l_topic[index_]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=make_data(range(len(train),len(train)+len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 191s 25us/step - loss: 0.4425 - acc: 0.8227 - val_loss: 0.4392 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43923, saving model to lstur_nofea_model0.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 182s 24us/step - loss: 0.4357 - acc: 0.8230 - val_loss: 0.4356 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43923 to 0.43562, saving model to lstur_nofea_model0.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 178s 23us/step - loss: 0.4323 - acc: 0.8232 - val_loss: 0.4341 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43562 to 0.43412, saving model to lstur_nofea_model0.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 175s 23us/step - loss: 0.4291 - acc: 0.8235 - val_loss: 0.4326 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43412 to 0.43256, saving model to lstur_nofea_model0.h5\n",
      "Epoch 5/10\n",
      "7591329/7591329 [==============================] - 175s 23us/step - loss: 0.4260 - acc: 0.8240 - val_loss: 0.4333 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43256\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 6/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4215 - acc: 0.8252 - val_loss: 0.4310 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43256 to 0.43098, saving model to lstur_nofea_model0.h5\n",
      "Epoch 7/10\n",
      "7591329/7591329 [==============================] - 177s 23us/step - loss: 0.4180 - acc: 0.8264 - val_loss: 0.4311 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43098\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 8/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4132 - acc: 0.8282 - val_loss: 0.4310 - val_acc: 0.8225\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43098\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00008: early stopping\n",
      "val_auc: 0.6959959662500637\n",
      "1\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 177s 23us/step - loss: 0.4428 - acc: 0.8227 - val_loss: 0.4402 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44015, saving model to lstur_nofea_model1.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 176s 23us/step - loss: 0.4360 - acc: 0.8230 - val_loss: 0.4358 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44015 to 0.43578, saving model to lstur_nofea_model1.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4325 - acc: 0.8232 - val_loss: 0.4342 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43578 to 0.43424, saving model to lstur_nofea_model1.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4294 - acc: 0.8235 - val_loss: 0.4327 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43424 to 0.43266, saving model to lstur_nofea_model1.h5\n",
      "Epoch 5/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4263 - acc: 0.8241 - val_loss: 0.4323 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43266 to 0.43229, saving model to lstur_nofea_model1.h5\n",
      "Epoch 6/10\n",
      "7591329/7591329 [==============================] - 172s 23us/step - loss: 0.4231 - acc: 0.8249 - val_loss: 0.4313 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43229 to 0.43127, saving model to lstur_nofea_model1.h5\n",
      "Epoch 7/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4202 - acc: 0.8260 - val_loss: 0.4309 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43127 to 0.43090, saving model to lstur_nofea_model1.h5\n",
      "Epoch 8/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4174 - acc: 0.8270 - val_loss: 0.4326 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43090\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 9/10\n",
      "7591329/7591329 [==============================] - 173s 23us/step - loss: 0.4127 - acc: 0.8288 - val_loss: 0.4316 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43090\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 00009: early stopping\n",
      "val_auc: 0.6961347025202749\n",
      "2\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 172s 23us/step - loss: 0.4427 - acc: 0.8226 - val_loss: 0.4377 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43768, saving model to lstur_nofea_model2.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 171s 23us/step - loss: 0.4358 - acc: 0.8230 - val_loss: 0.4358 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43768 to 0.43577, saving model to lstur_nofea_model2.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4324 - acc: 0.8232 - val_loss: 0.4335 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43577 to 0.43346, saving model to lstur_nofea_model2.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4291 - acc: 0.8235 - val_loss: 0.4321 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43346 to 0.43214, saving model to lstur_nofea_model2.h5\n",
      "Epoch 5/10\n",
      "7591329/7591329 [==============================] - 169s 22us/step - loss: 0.4260 - acc: 0.8242 - val_loss: 0.4321 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43214 to 0.43207, saving model to lstur_nofea_model2.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 6/10\n",
      "7591329/7591329 [==============================] - 171s 23us/step - loss: 0.4213 - acc: 0.8257 - val_loss: 0.4308 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43207 to 0.43080, saving model to lstur_nofea_model2.h5\n",
      "Epoch 7/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4177 - acc: 0.8269 - val_loss: 0.4311 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43080\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 8/10\n",
      "7591329/7591329 [==============================] - 170s 22us/step - loss: 0.4128 - acc: 0.8288 - val_loss: 0.4311 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43080\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "Epoch 00008: early stopping\n",
      "val_auc: 0.6959837786736491\n",
      "3\n",
      "Train on 7591330 samples, validate on 1897832 samples\n",
      "Epoch 1/10\n",
      "7591330/7591330 [==============================] - 172s 23us/step - loss: 0.4424 - acc: 0.8227 - val_loss: 0.4379 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43791, saving model to lstur_nofea_model3.h5\n",
      "Epoch 2/10\n",
      "7591330/7591330 [==============================] - 172s 23us/step - loss: 0.4357 - acc: 0.8230 - val_loss: 0.4349 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43791 to 0.43495, saving model to lstur_nofea_model3.h5\n",
      "Epoch 3/10\n",
      "7591330/7591330 [==============================] - 169s 22us/step - loss: 0.4322 - acc: 0.8232 - val_loss: 0.4346 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43495 to 0.43457, saving model to lstur_nofea_model3.h5\n",
      "Epoch 4/10\n",
      "7591330/7591330 [==============================] - 171s 23us/step - loss: 0.4289 - acc: 0.8236 - val_loss: 0.4326 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43457 to 0.43256, saving model to lstur_nofea_model3.h5\n",
      "Epoch 5/10\n",
      "7591330/7591330 [==============================] - 169s 22us/step - loss: 0.4257 - acc: 0.8243 - val_loss: 0.4314 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43256 to 0.43144, saving model to lstur_nofea_model3.h5\n",
      "Epoch 6/10\n",
      "7591330/7591330 [==============================] - 173s 23us/step - loss: 0.4225 - acc: 0.8252 - val_loss: 0.4308 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43144 to 0.43082, saving model to lstur_nofea_model3.h5\n",
      "Epoch 7/10\n",
      "7591330/7591330 [==============================] - 170s 22us/step - loss: 0.4196 - acc: 0.8262 - val_loss: 0.4328 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43082\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 8/10\n",
      "7591330/7591330 [==============================] - 170s 22us/step - loss: 0.4147 - acc: 0.8279 - val_loss: 0.4310 - val_acc: 0.8224\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43082\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 00008: early stopping\n",
      "val_auc: 0.6966664228985495\n",
      "4\n",
      "Train on 7591331 samples, validate on 1897831 samples\n",
      "Epoch 1/10\n",
      "7591331/7591331 [==============================] - 178s 23us/step - loss: 0.4425 - acc: 0.8226 - val_loss: 0.4382 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43816, saving model to lstur_nofea_model4.h5\n",
      "Epoch 2/10\n",
      "7591331/7591331 [==============================] - 174s 23us/step - loss: 0.4358 - acc: 0.8230 - val_loss: 0.4353 - val_acc: 0.8229\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43816 to 0.43526, saving model to lstur_nofea_model4.h5\n",
      "Epoch 3/10\n",
      "7591331/7591331 [==============================] - 172s 23us/step - loss: 0.4324 - acc: 0.8232 - val_loss: 0.4338 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43526 to 0.43378, saving model to lstur_nofea_model4.h5\n",
      "Epoch 4/10\n",
      "7591331/7591331 [==============================] - 173s 23us/step - loss: 0.4292 - acc: 0.8235 - val_loss: 0.4327 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.43378 to 0.43274, saving model to lstur_nofea_model4.h5\n",
      "Epoch 5/10\n",
      "7591331/7591331 [==============================] - 172s 23us/step - loss: 0.4260 - acc: 0.8242 - val_loss: 0.4316 - val_acc: 0.8231\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43274 to 0.43165, saving model to lstur_nofea_model4.h5\n",
      "Epoch 6/10\n",
      "7591331/7591331 [==============================] - 172s 23us/step - loss: 0.4230 - acc: 0.8251 - val_loss: 0.4311 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43165 to 0.43109, saving model to lstur_nofea_model4.h5\n",
      "Epoch 7/10\n",
      "7591331/7591331 [==============================] - 177s 23us/step - loss: 0.4200 - acc: 0.8261 - val_loss: 0.4311 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43109 to 0.43106, saving model to lstur_nofea_model4.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 8/10\n",
      "7591331/7591331 [==============================] - 174s 23us/step - loss: 0.4153 - acc: 0.8278 - val_loss: 0.4311 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43106\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "Epoch 00008: early stopping\n",
      "val_auc: 0.6948296389189167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    K.clear_session()\n",
    "    filepath = \"lstur_nofea_model%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=2,verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    print(index)\n",
    "    nnmodel = LSTUR()\n",
    "\n",
    "    train_y, test_y =target[train_index], target[test_index]\n",
    "    train_x=make_data(train_index)\n",
    "    test_x=make_data(test_index)\n",
    "\n",
    "    history = nnmodel.fit(train_x, train_y, batch_size=2048, epochs=10, verbose=1, validation_data=(test_x, test_y),callbacks=callbacks,)\n",
    "    nnmodel.load_weights(filepath)\n",
    "    \n",
    "\n",
    "    sub1 += nnmodel.predict(test_data, batch_size=2048)\n",
    "    oof_pref1[test_index] = nnmodel.predict(test_x, batch_size=2048)\n",
    "    count += 1\n",
    "    print('val_auc:',metrics.roc_auc_score(test_y,oof_pref1[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1/=5\n",
    "stacking=pd.concat([pd.DataFrame(oof_pref1),pd.DataFrame(sub1)])\n",
    "stacking.columns=['nn_LSTUR']\n",
    "stacking=stacking.reset_index(drop=True)\n",
    "stacking.to_hdf('stacking_nn_LSTUR_cv68.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = np.zeros((test.shape[0],1 ))\n",
    "oof_pref2 = np.zeros((train.shape[0], 1))\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "start\n",
      "val_auc: 0.6959959662500637\n",
      "1\n",
      "start\n",
      "val_auc: 0.6961347025202749\n",
      "2\n",
      "start\n",
      "val_auc: 0.6959837786736491\n",
      "3\n",
      "start\n",
      "val_auc: 0.6966664228985495\n",
      "4\n",
      "start\n",
      "val_auc: 0.6948296389189167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    K.clear_session()\n",
    "    filepath = \"lstur_nofea_model%d.h5\" % index\n",
    "    model = LSTUR()\n",
    "    print(index)\n",
    "    train_x=make_data(train_index)\n",
    "    test_x=make_data(test_index)\n",
    "    train_y, test_y =target[train_index], target[test_index]\n",
    "    print('start')\n",
    "    model.load_weights(filepath)\n",
    "    sub2 += model.predict(make_data(range(len(train),len(train)+len(test))), batch_size=4000)\n",
    "    oof_pref2[test_index] = model.predict(test_x, batch_size=4000)\n",
    "    count += 1\n",
    "    print('val_auc:',metrics.roc_auc_score(test_y,oof_pref2[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking=pd.concat([pd.DataFrame(oof_pref2),pd.DataFrame(sub2/5)])\n",
    "stacking.columns=['lstur_no_feature']\n",
    "stacking=stacking.reset_index(drop=True)\n",
    "stacking.to_hdf('stackinglstur_wnf.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
