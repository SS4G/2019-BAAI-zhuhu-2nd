{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "这个是nn代码，可以直接运行，生成结果文件final_esim_fea.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "import gc\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embedding=np.load('esim_topic_embedding.npy')\n",
    "word_embedding=np.load('esim_word_embedding.npy')\n",
    "\n",
    "w_topic=np.load('w_topic.npy')\n",
    "last_6_tpoic=np.load('last_6_tpoic.npy')\n",
    "l_topic=np.load('l_topic.npy')\n",
    "u_topic=np.load('u_topic.npy')\n",
    "qtopic_10=np.load('qtopic_10.npy')\n",
    "qtopic_50=np.load('qtopic_50.npy')\n",
    "\n",
    "qtitlec_1=np.load('qtitlec_1.npy')\n",
    "qtitlec=np.load('qtitlec.npy')\n",
    "qinfoc=np.load('qinfoc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.load(\"all_data_final.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shape=feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_2_0926.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "target=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.layers import InputSpec, Layer, Input, Dense, merge, Conv1D\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "def unchanged_shape(input_shape):\n",
    "    \"Function for Lambda layer\"\n",
    "    return input_shape\n",
    "def substract(input_1, input_2):\n",
    "    \"Substract element-wise\"\n",
    "    neg_input_2 = Lambda(lambda x: -x, output_shape=unchanged_shape)(input_2)\n",
    "    out_ = Add()([input_1, neg_input_2])\n",
    "    return out_\n",
    "def submult(input_1, input_2):\n",
    "    \"Get multiplication and subtraction then concatenate results\"\n",
    "    mult = Multiply()([input_1, input_2])\n",
    "    sub = substract(input_1, input_2)\n",
    "    out_= Concatenate()([sub, mult])\n",
    "    return out_\n",
    "def apply_multiple(input_, layers):\n",
    "    \"Apply layers to input then concatenate result\"\n",
    "    if not len(layers) > 1:\n",
    "        raise ValueError('Layers list should contain more than 1 layer')\n",
    "    else:\n",
    "        agg_ = []\n",
    "        for layer in layers:\n",
    "            agg_.append(layer(input_))\n",
    "        out_ = Concatenate()(agg_)\n",
    "    return out_\n",
    "def time_distributed(input_, layers):\n",
    "    \"Apply a list of layers in TimeDistributed mode\"\n",
    "    out_ = []\n",
    "    node_ = input_\n",
    "    for layer_ in layers:\n",
    "        node_ = TimeDistributed(layer_)(node_)\n",
    "    out_ = node_\n",
    "    return out_\n",
    "def soft_attention_alignment(input_1, input_2):\n",
    "    \"Align text representation with neural soft attention\"\n",
    "    attention = Dot(axes=-1)([input_1, input_2])\n",
    "    w_att_1 = Lambda(lambda x: softmax(x, axis=1),\n",
    "                     output_shape=unchanged_shape)(attention)\n",
    "    w_att_2 = Permute((2,1))(Lambda(lambda x: softmax(x, axis=2),\n",
    "                             output_shape=unchanged_shape)(attention))\n",
    "    in1_aligned = Dot(axes=1)([w_att_1, input_1])\n",
    "    in2_aligned = Dot(axes=1)([w_att_2, input_2])\n",
    "    return in1_aligned, in2_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnmodel(word_embedding=word_embedding,topic_embedding=topic_embedding):\n",
    "\n",
    "###########################################q_topic l_topic match\n",
    "    topic_embedding_50 = Embedding(\n",
    "        input_dim=topic_embedding.shape[0],\n",
    "        output_dim=topic_embedding.shape[1],\n",
    "        weights=[topic_embedding],\n",
    "        input_length=50,\n",
    "        trainable=False\n",
    "    ) \n",
    "    q_topic_50 = Input(shape=(50,), name=\"q_topic_50\")\n",
    "    l_topic    = Input(shape=(50,), name=\"l_topic\")\n",
    "    emb1 = topic_embedding_50(q_topic_50)\n",
    "    emb2 = topic_embedding_50(l_topic)\n",
    "    topic_encode = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    t1_encoded = topic_encode(emb1)\n",
    "    t2_encoded = topic_encode(emb2)\n",
    "    t1_aligned, t2_aligned = soft_attention_alignment(t1_encoded, t2_encoded)\n",
    "    t1_combined = Concatenate()([t1_encoded, t2_aligned, submult(t1_encoded, t2_aligned)])\n",
    "    t2_combined = Concatenate()([t2_encoded, t1_aligned, submult(t2_encoded, t1_aligned)]) \n",
    "    topic_compose = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    t1_compare = topic_compose(t1_combined)\n",
    "    t2_compare = topic_compose(t2_combined)\n",
    "    t1_rep = apply_multiple(t1_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "    t2_rep = apply_multiple(t2_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "###################################################################q_topic u_topic\n",
    "    topic_embedding_10 = Embedding(\n",
    "            input_dim=topic_embedding.shape[0],\n",
    "            output_dim=topic_embedding.shape[1],\n",
    "            weights=[topic_embedding],\n",
    "            input_length=10,\n",
    "            trainable=False\n",
    "        ) \n",
    "    q_topic_10 = Input(shape=(10,), name=\"q_topic_10\")\n",
    "    u_topic = Input(shape=(10,), name=\"u_topic\")\n",
    "    u_topic_emb1 = topic_embedding_10(u_topic)\n",
    "    q_topic_emb1 = topic_embedding_10(q_topic_10)\n",
    "    topic_encode2 = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    t1_encoded2 = topic_encode2(q_topic_emb1)\n",
    "    t2_encoded2= topic_encode2(u_topic_emb1)\n",
    "    t1_aligned2, t2_aligned2 = soft_attention_alignment(t1_encoded2, t2_encoded2)\n",
    "    t1_combined2 = Concatenate()([t1_encoded2, t2_aligned2, submult(t1_encoded2, t2_aligned2)])\n",
    "    t2_combined2 = Concatenate()([t2_encoded2, t1_aligned2, submult(t2_encoded2, t1_aligned2)]) \n",
    "    topic_compose2 = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    t1_compare2 = topic_compose2(t1_combined2)\n",
    "    t2_compare2 = topic_compose2(t2_combined2)\n",
    "    t1_rep2 = apply_multiple(t1_compare2, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "    t2_rep2 = apply_multiple(t2_compare2, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "########################################################################## w_topic\n",
    "    w_topic = Input(shape=(10,1), name=\"w_topic\")\n",
    "    u_topic_weight=Multiply()([u_topic_emb1, w_topic])\n",
    "    \n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv1a = conv1(u_topic_weight)\n",
    "    conv1b = conv1(q_topic_emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "    conv3a = conv3(u_topic_weight)\n",
    "    conv3b = conv3(q_topic_emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "    conv5a = conv5(u_topic_weight)\n",
    "    conv5b = conv5(q_topic_emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "    mergea = concatenate([glob1a, glob3a,  glob5a])\n",
    "    mergeb = concatenate([glob1b, glob3b,  glob5b])\n",
    "    difftw = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(2 * 128 + 1*32,))([mergea, mergeb])\n",
    "    multw = Lambda(lambda x: x[0] * x[1], output_shape=(2 * 128 + 1*32,))([mergea, mergeb])\n",
    "    wtmatch=concatenate([difftw,multw,mergea])\n",
    "###########################################qtitlec  qtitlec_1 match\n",
    "    qtitlec = Input(shape=(20,), name=\"qtitlec\")\n",
    "    qtitlec_1 = Input(shape=(20,), name=\"qtitlec_1\")        \n",
    "    title_word_embedding = Embedding(\n",
    "        input_dim=word_embedding.shape[0],\n",
    "        output_dim=word_embedding.shape[1],\n",
    "        weights=[word_embedding],\n",
    "        input_length=20,\n",
    "        trainable=False\n",
    "    )\n",
    "    q1_embed = title_word_embedding(qtitlec)\n",
    "    q2_embed = title_word_embedding(qtitlec_1)\n",
    "    encode = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    q1_encoded = encode(q1_embed)\n",
    "    q2_encoded = encode(q2_embed)\n",
    "    q1_aligned, q2_aligned = soft_attention_alignment(q1_encoded, q2_encoded)\n",
    "    q1_combined = Concatenate()([q1_encoded, q2_aligned, submult(q1_encoded, q2_aligned)])\n",
    "    q2_combined = Concatenate()([q2_encoded, q1_aligned, submult(q2_encoded, q1_aligned)]) \n",
    "    compose = Bidirectional(CuDNNLSTM(64, return_sequences=True))\n",
    "    q1_compare = compose(q1_combined)\n",
    "    q2_compare = compose(q2_combined)\n",
    "    q1_rep = apply_multiple(q1_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "    q2_rep = apply_multiple(q2_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "#############################################################qinfoc\n",
    "    qinfoc = Input(shape=(40,), name=\"qinfoc\")\n",
    "    qinfoc_word_embedding = Embedding(\n",
    "        input_dim=word_embedding.shape[0],\n",
    "        output_dim=word_embedding.shape[1],\n",
    "        weights=[word_embedding],\n",
    "        input_length=40,\n",
    "        trainable=False\n",
    "    )\n",
    "    qinfocconv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    qinfocconv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    qinfocconv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    qinfoc_embed = qinfoc_word_embedding(qinfoc)\n",
    "    info_conv1a = conv1(qinfoc_embed)\n",
    "    info_glob1a = GlobalAveragePooling1D()(info_conv1a)\n",
    "    info_glob1b = GlobalMaxPool1D()(info_conv1a)\n",
    "    info_conv3a = conv3(qinfoc_embed)\n",
    "    info_glob3a = GlobalAveragePooling1D()(info_conv3a)\n",
    "    info_glob3b = GlobalMaxPool1D()(info_conv3a)\n",
    "    info_conv5a = conv5(qinfoc_embed)\n",
    "    info_glob5a = GlobalAveragePooling1D()(info_conv5a)\n",
    "    info_glob5b = GlobalMaxPool1D()(info_conv5a)\n",
    "    info_merge = concatenate([info_glob1a, info_glob3a,info_glob5a,info_glob1b,info_glob3b,info_glob5b])\n",
    "################################################feature\n",
    "    feature = Input(shape=(feature_shape,), name=\"feature\")\n",
    "    fea1 = Dense(1024, activation='relu')(feature)\n",
    "    dense = Dropout(0.3)(fea1)\n",
    "    dense = Dense(512, activation='relu')(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "###############################################################\n",
    "    last_6_tpoic = Input(shape=(50,), name=\"last_6_tpoic\")\n",
    "    emb1t = topic_embedding_50(q_topic_50)\n",
    "    emb2t = topic_embedding_50(last_6_tpoic)\n",
    "    qtcconv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    qtcconv2 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    qtcconv3 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv1at = qtcconv1(emb1t)\n",
    "    conv1bt = qtcconv1(emb2t)\n",
    "    glob1at = GlobalAveragePooling1D()(conv1at)\n",
    "    glob1bt = GlobalAveragePooling1D()(conv1bt)\n",
    "    conv2at = qtcconv2(emb1t)\n",
    "    conv2bt = qtcconv2(emb2t)\n",
    "    glob2at = GlobalAveragePooling1D()(conv2at)\n",
    "    glob2bt = GlobalAveragePooling1D()(conv2bt)\n",
    "    conv3at = qtcconv3(emb1t)\n",
    "    conv3bt = qtcconv3(emb2t)\n",
    "    glob3at = GlobalAveragePooling1D()(conv3at)\n",
    "    glob3bt = GlobalAveragePooling1D()(conv3bt)\n",
    "    mergeat = concatenate([glob1at, glob2at,  glob3at])\n",
    "    mergebt = concatenate([glob1bt, glob2bt,  glob3bt])\n",
    "    difft = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(2 * 128 + 1*32,))([mergeat, mergebt])\n",
    "    mult = Lambda(lambda x: x[0] * x[1], output_shape=(2 * 128 + 1*32,))([mergeat, mergebt])\n",
    "###########################################main layer\n",
    "    \n",
    "    \n",
    "    merge =concatenate([q1_rep,q2_rep,t1_rep,t2_rep,wtmatch,info_merge,t1_rep2,t2_rep2,difft,mult,dense])#diff,mul])#,categ,,dense\n",
    "    main_l = Dense(1024, activation='relu', )(merge)\n",
    "    main_l = Dropout(0.3)(main_l)\n",
    "    main_l = Dense(512, activation='relu', )(main_l)\n",
    "    \n",
    "    output = Dense(1,activation='sigmoid') (main_l)\n",
    "\n",
    "    model = Model(inputs=[qtitlec,qtitlec_1,q_topic_10,l_topic,u_topic,w_topic,qinfoc,q_topic_50,last_6_tpoic,feature], outputs=output)#feature\n",
    "\n",
    "    model.compile(optimizer ='adam',\n",
    "                  loss= 'binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(index_):\n",
    "    data=dict()\n",
    "    data['w_topic']=w_topic_[index_]\n",
    "    data['last_6_tpoic']=last_6_tpoic[index_]\n",
    "    data['q_topic_10']=qtopic_10[index_]\n",
    "    data['q_topic_50']=qtopic_50[index_]\n",
    "    data['qtitlec_1']=qtitlec_1[index_]\n",
    "    data['qtitlec']=qtitlec[index_]\n",
    "    data['qinfoc']=qinfoc[index_]\n",
    "    data['feature']=feature[index_]\n",
    "    data['l_topic']=l_topic[index_]\n",
    "    data['u_topic']=u_topic[index_]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_topic_=[]\n",
    "for i in range(len(w_topic)):\n",
    "    w_topic_.append(w_topic[i].reshape(-1,1))\n",
    "w_topic_=np.array(w_topic_)\n",
    "del w_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = np.zeros((test.shape[0],1 ))\n",
    "oof_pref1 = np.zeros((train.shape[0], 1))\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1217 17:23:10.740691 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:92: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1217 17:23:10.742725 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1217 17:23:10.766636 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1217 17:23:10.768985 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1217 17:23:10.771517 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1217 17:23:17.533857 139812632864512 deprecation.py:506] From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1217 17:23:17.739103 139812632864512 deprecation_wrapper.py:119] From /root/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1217 17:23:17.765181 139812632864512 deprecation.py:323] From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "start\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 1099s 145us/step - loss: 0.3572 - acc: 0.8457 - val_loss: 0.3318 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33176, saving model to esim_word_nofea_0.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 1074s 141us/step - loss: 0.3291 - acc: 0.8565 - val_loss: 0.3213 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33176 to 0.32125, saving model to esim_word_nofea_0.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 1077s 142us/step - loss: 0.3205 - acc: 0.8604 - val_loss: 0.3203 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32125 to 0.32032, saving model to esim_word_nofea_0.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 1059s 139us/step - loss: 0.3147 - acc: 0.8628 - val_loss: 0.3158 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32032 to 0.31577, saving model to esim_word_nofea_0.h5\n",
      "Epoch 5/10\n",
      "7591329/7591329 [==============================] - 1057s 139us/step - loss: 0.3100 - acc: 0.8648 - val_loss: 0.3137 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31577 to 0.31367, saving model to esim_word_nofea_0.h5\n",
      "Epoch 6/10\n",
      "7591329/7591329 [==============================] - 1036s 137us/step - loss: 0.3055 - acc: 0.8668 - val_loss: 0.3168 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31367\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 00006: early stopping\n",
      "val_auc: 0.8742398966708108\n",
      "1\n",
      "start\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 1045s 138us/step - loss: 0.3591 - acc: 0.8456 - val_loss: 0.3305 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33047, saving model to esim_word_nofea_1.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 1028s 135us/step - loss: 0.3289 - acc: 0.8567 - val_loss: 0.3217 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33047 to 0.32170, saving model to esim_word_nofea_1.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 1029s 136us/step - loss: 0.3201 - acc: 0.8605 - val_loss: 0.3167 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32170 to 0.31667, saving model to esim_word_nofea_1.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 1025s 135us/step - loss: 0.3143 - acc: 0.8630 - val_loss: 0.3170 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31667\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 00004: early stopping\n",
      "val_auc: 0.8711874599629982\n",
      "2\n",
      "start\n",
      "Train on 7591329 samples, validate on 1897833 samples\n",
      "Epoch 1/10\n",
      "7591329/7591329 [==============================] - 1042s 137us/step - loss: 0.3587 - acc: 0.8459 - val_loss: 0.3304 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33041, saving model to esim_word_nofea_2.h5\n",
      "Epoch 2/10\n",
      "7591329/7591329 [==============================] - 1026s 135us/step - loss: 0.3287 - acc: 0.8568 - val_loss: 0.3249 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33041 to 0.32493, saving model to esim_word_nofea_2.h5\n",
      "Epoch 3/10\n",
      "7591329/7591329 [==============================] - 1027s 135us/step - loss: 0.3202 - acc: 0.8605 - val_loss: 0.3189 - val_acc: 0.8629\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32493 to 0.31893, saving model to esim_word_nofea_2.h5\n",
      "Epoch 4/10\n",
      "7591329/7591329 [==============================] - 1029s 135us/step - loss: 0.3146 - acc: 0.8629 - val_loss: 0.3156 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31893 to 0.31555, saving model to esim_word_nofea_2.h5\n",
      "Epoch 5/10\n",
      "7591329/7591329 [==============================] - 1029s 136us/step - loss: 0.3099 - acc: 0.8649 - val_loss: 0.3151 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31555 to 0.31510, saving model to esim_word_nofea_2.h5\n",
      "Epoch 6/10\n",
      "7591329/7591329 [==============================] - 1028s 135us/step - loss: 0.3054 - acc: 0.8669 - val_loss: 0.3130 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31510 to 0.31303, saving model to esim_word_nofea_2.h5\n",
      "Epoch 7/10\n",
      "7591329/7591329 [==============================] - 1027s 135us/step - loss: 0.3014 - acc: 0.8687 - val_loss: 0.3143 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31303\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 00007: early stopping\n",
      "val_auc: 0.8754578303654468\n",
      "3\n",
      "start\n",
      "Train on 7591330 samples, validate on 1897832 samples\n",
      "Epoch 1/10\n",
      "7591330/7591330 [==============================] - 1048s 138us/step - loss: 0.3592 - acc: 0.8456 - val_loss: 0.3300 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33005, saving model to esim_word_nofea_3.h5\n",
      "Epoch 2/10\n",
      "7591330/7591330 [==============================] - 1032s 136us/step - loss: 0.3288 - acc: 0.8566 - val_loss: 0.3232 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33005 to 0.32319, saving model to esim_word_nofea_3.h5\n",
      "Epoch 3/10\n",
      "7591330/7591330 [==============================] - 1031s 136us/step - loss: 0.3201 - acc: 0.8605 - val_loss: 0.3182 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32319 to 0.31818, saving model to esim_word_nofea_3.h5\n",
      "Epoch 4/10\n",
      "7591330/7591330 [==============================] - 1030s 136us/step - loss: 0.3145 - acc: 0.8630 - val_loss: 0.3158 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31818 to 0.31583, saving model to esim_word_nofea_3.h5\n",
      "Epoch 5/10\n",
      "7591330/7591330 [==============================] - 1028s 135us/step - loss: 0.3096 - acc: 0.8650 - val_loss: 0.3131 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31583 to 0.31311, saving model to esim_word_nofea_3.h5\n",
      "Epoch 6/10\n",
      "7591330/7591330 [==============================] - 1025s 135us/step - loss: 0.3053 - acc: 0.8670 - val_loss: 0.3127 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31311 to 0.31273, saving model to esim_word_nofea_3.h5\n",
      "Epoch 7/10\n",
      "7591330/7591330 [==============================] - 1026s 135us/step - loss: 0.3012 - acc: 0.8688 - val_loss: 0.3127 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31273 to 0.31268, saving model to esim_word_nofea_3.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 00007: early stopping\n",
      "val_auc: 0.8743086251574615\n",
      "4\n",
      "start\n",
      "Train on 7591331 samples, validate on 1897831 samples\n",
      "Epoch 1/10\n",
      "7591331/7591331 [==============================] - 1047s 138us/step - loss: 0.3538 - acc: 0.8463 - val_loss: 0.3352 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33518, saving model to esim_word_nofea_4.h5\n",
      "Epoch 2/10\n",
      "7591331/7591331 [==============================] - 1029s 135us/step - loss: 0.3285 - acc: 0.8568 - val_loss: 0.3246 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33518 to 0.32463, saving model to esim_word_nofea_4.h5\n",
      "Epoch 3/10\n",
      "7591331/7591331 [==============================] - 1028s 135us/step - loss: 0.3203 - acc: 0.8604 - val_loss: 0.3205 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32463 to 0.32050, saving model to esim_word_nofea_4.h5\n",
      "Epoch 4/10\n",
      "7591331/7591331 [==============================] - 1027s 135us/step - loss: 0.3148 - acc: 0.8627 - val_loss: 0.3181 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32050 to 0.31809, saving model to esim_word_nofea_4.h5\n",
      "Epoch 5/10\n",
      "7591331/7591331 [==============================] - 1030s 136us/step - loss: 0.3103 - acc: 0.8648 - val_loss: 0.3226 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31809\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "Epoch 00005: early stopping\n",
      "val_auc: 0.8722055816337654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    K.clear_session()\n",
    "    filepath = \"esim_word_nofea_%d.h5\" % count\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=1,verbose=1, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "    \n",
    "    model = nnmodel()\n",
    "    print(index)\n",
    "    train_x=make_data(train_index)\n",
    "    test_x=make_data(test_index)\n",
    "    train_y, test_y =target[train_index], target[test_index]\n",
    "    print('start')\n",
    "    history = model.fit(train_x,train_y, batch_size=2048, epochs=10, verbose=1, validation_data=(test_x, test_y),callbacks=callbacks,)\n",
    "    model.load_weights(filepath)\n",
    "    sub1 += model.predict(make_data(range(len(train),len(train)+len(test))), batch_size=4000)\n",
    "    oof_pref1[test_index] = model.predict(test_x, batch_size=4000)\n",
    "    count += 1\n",
    "    print('val_auc:',metrics.roc_auc_score(test_y,oof_pref1[test_index]))#0.42480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = np.zeros((test.shape[0],1 ))\n",
    "oof_pref2 = np.zeros((train.shape[0], 1))\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=1996, shuffle=True)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, target)):\n",
    "    K.clear_session()\n",
    "    filepath = \"esim_word_nofea_%d.h5\" % index\n",
    "    model = nnmodel()\n",
    "    print(index)\n",
    "    train_x=make_data(train_index)\n",
    "    test_x=make_data(test_index)\n",
    "    train_y, test_y =target[train_index], target[test_index]\n",
    "    print('start')\n",
    "    model.load_weights(filepath)\n",
    "    sub2 += model.predict(make_data(range(len(train),len(train)+len(test))), batch_size=4000)\n",
    "    oof_pref2[test_index] = model.predict(test_x, batch_size=4000)\n",
    "    count += 1\n",
    "    print('val_auc:',metrics.roc_auc_score(test_y,oof_pref2[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking=pd.concat([pd.DataFrame(oof_pref1),pd.DataFrame(sub1/5)])\n",
    "stacking.columns=['esim_fea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking=stacking.reset_index(drop=True)\n",
    "stacking.to_hdf('final_esim_fea.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034193715641305855"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1/=5\n",
    "sub1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
