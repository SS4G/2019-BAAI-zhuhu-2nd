{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_info=pd.read_csv('question_info.txt',sep='\\s+',names=['qid','qtime','qtitle','qtitlec','qinfo','qinfoc','qtopic'])\n",
    "ques_info=ques_info[['qid','qtitle','qinfo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "embed_size=64\n",
    "tokenizer=Tokenizer(lower=False,char_level=False,split=',')\n",
    "tokenizer.fit_on_texts(ques_info['qtitle'].append(ques_info['qinfo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('char_tokenizer.pickle', 'wb') as f:\n",
    "      pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "1\n",
      "oov num is 1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open( name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "topic_w2v=load_obj('single_word_vectors_64d')\n",
    "\n",
    "max_features=len(tokenizer.word_index)+1\n",
    "char_embedding=np.zeros((max_features,embed_size))\n",
    "num_oov=0\n",
    "for word in tokenizer.word_index:\n",
    "    try:\n",
    "        char_embedding[tokenizer.word_index[word]]=topic_w2v[word]['vector']\n",
    "    except:\n",
    "        print(tokenizer.word_index[word])\n",
    "        print(word)\n",
    "        np.random.seed(1997)\n",
    "        char_embedding[tokenizer.word_index[word]]=np.random.normal(1,1,64)\n",
    "        num_oov+=1\n",
    "print('oov num is '+str(num_oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"esim_char_embedding.npy\", char_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################一下为复赛需要运行的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ques_info=pd.read_csv('question_info.txt',sep='\\s+',names=['qid','qtime','qtitle','qtitlec','qinfo','qinfoc','qtopic'])\n",
    "ques_info=ques_info[['qid','qtitle','qinfo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_2_0926.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "all_data=pd.concat([train[['qid','uid']],test[['qid','uid']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_answer=pd.read_hdf('last_answerlist_6.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_answer=last_answer[['qid','uid','qid_1']]\n",
    "last_answer=last_answer.drop_duplicates(['qid','uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.merge(ques_info,on=['qid'],how='left')\n",
    "all_data=all_data.merge(last_answer,on=['qid','uid'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_info.columns=['qid_1','qtitle_1','qinfo_1']\n",
    "all_data=all_data.merge(ques_info,on=['qid_1'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.fillna('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('char_tokenizer.pickle', 'rb') as f:\n",
    "      tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qtitle  =tokenizer.texts_to_sequences(all_data['qtitle'])\n",
    "qinfo=tokenizer.texts_to_sequences(all_data['qinfo'])\n",
    "qtitle_1  =tokenizer.texts_to_sequences(all_data['qtitle_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtitle=pad_sequences(qtitle,maxlen=30,value=0)\n",
    "qinfo=pad_sequences(qinfo,maxlen=50,value=0)\n",
    "qtitle_1=pad_sequences(qtitle_1,maxlen=30,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"qtitle.npy\", qtitle)\n",
    "np.save(\"qinfo.npy\", qinfo)\n",
    "np.save(\"qtitle_1.npy\", qtitle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_info=pd.read_csv('question_info.txt',sep='\\s+',names=['qid','qtime','qtitle','qtitlec','qinfo','qinfoc','qtopic'])\n",
    "ques_info=ques_info[['qid','qtopic']]\n",
    "last_answer=pd.read_hdf('last_answerlist_6_topic.h5', key='data')\n",
    "train=pd.read_csv('invite_info.txt',sep='\\s+',names=['qid','uid','time','target'])\n",
    "test=pd.read_csv('invite_info_evaluate_1.txt',sep='\\s+',names=['qid','uid','time'])\n",
    "train=train[['qid','uid']]\n",
    "test=test[['qid','uid']]\n",
    "all_data=pd.concat([train,test])\n",
    "member_info=pd.read_csv('member_info.txt',sep='\\s+',names=['uid','sex','key_word','num_level','hot_level','regis_type','regis_platform',\n",
    "                                                          'look_freq','a','b','c','d','e','A','B','C','D','E','salt','l_topic','topic_n'])\n",
    "member_info=member_info[['uid','l_topic','topic_n']]\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_topic(x):\n",
    "    x=x.split(',')\n",
    "    return ','.join([i.split(':')[0] for i in x])\n",
    "member_info['u_topic']=member_info['topic_n'].apply(lambda x:split_topic(x))\n",
    "def split_weight(x):\n",
    "    if x!='-1':\n",
    "        x=x.split(',')\n",
    "        return [float(i.split(':')[1]) for i in x]\n",
    "    else:return [0]*10\n",
    "member_info['w_topic']=member_info['topic_n'].apply(lambda x:split_weight(x))\n",
    "member_info.loc[member_info['l_topic']=='-1','l_topic']=member_info[member_info['l_topic']=='-1']['u_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7573780/7573780 [00:18<00:00, 413189.01it/s]\n"
     ]
    }
   ],
   "source": [
    "last_answer['last_6_tpoic']=last_answer['last_6_tpoic'].progress_apply(lambda x:','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.merge(ques_info,on='qid',how='left')\n",
    "all_data=all_data.merge(member_info,on='uid',how='left')\n",
    "all_data=all_data.merge(last_answer,on=['qid','uid'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.fillna('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('esim_topic_tokenizer.pickle', 'rb') as f:\n",
    "      tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['last_6_tpoic']=all_data['last_6_tpoic'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_topic  =tokenizer.texts_to_sequences(all_data['u_topic'])\n",
    "qtopic=tokenizer.texts_to_sequences(all_data['qtopic'])\n",
    "l_topic  =tokenizer.texts_to_sequences(all_data['l_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_6_tpoic  =tokenizer.texts_to_sequences(all_data['last_6_tpoic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_6_tpoic=pad_sequences(last_6_tpoic,maxlen=50,value=0)\n",
    "l_topic=pad_sequences(l_topic,maxlen=50,value=0)\n",
    "u_topic=pad_sequences(u_topic,maxlen=10,value=0)\n",
    "qtopic_10=pad_sequences(qtopic,maxlen=10,value=0)\n",
    "qtopic_50=pad_sequences(qtopic,maxlen=50,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_topic=pad_sequences(all_data['w_topic'].tolist(),maxlen=10,value=0, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10630845, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_topic = np.array(pd.DataFrame(w_topic).replace([np.inf, -np.inf], 1).fillna(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"last_6_tpoic.npy\", last_6_tpoic)\n",
    "np.save(\"l_topic.npy\", l_topic)\n",
    "np.save(\"u_topic.npy\", u_topic)\n",
    "np.save(\"qtopic_10.npy\", qtopic_10)\n",
    "np.save(\"qtopic_50.npy\", qtopic_50)\n",
    "np.save(\"w_topic.npy\", w_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
